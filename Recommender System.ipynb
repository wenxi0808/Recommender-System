{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89355511-ab46-4498-8e38-54702277e348",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Sample transactions_clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b966570b-e4dc-43d4-b093-a9762f99d0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "## construct credentials from service account key file\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    '/content/shu88-isom676-srvacct_srvacct.json') ## relative file path\n",
    "\n",
    "## construct a BigQuery client object\n",
    "client = bigquery.Client(credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec05421-8798-4a0a-b37e-e489492a86b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly strategied 50000 customers sample the transactions_clean table\n",
    "QUERY = (\n",
    "\"\"\"\n",
    "SELECT *\n",
    "FROM `machine_learning.transactions_clean`\n",
    "WHERE cust_id IN (\n",
    "  SELECT cust_id\n",
    "  FROM (\n",
    "    SELECT DISTINCT cust_id\n",
    "    FROM `machine_learning.transactions_clean`\n",
    "  ) AS UniqueCustomers\n",
    "  ORDER BY RAND()\n",
    "  LIMIT 50000\n",
    ")\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "query_job = client.query(QUERY)  # API request\n",
    "sample_transactions = query_job.to_dataframe()  # Converts the query result directly into a pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc297fb7-1f9a-4139-b4e6-4fad7c83da68",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# General EDA & Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ed7040-4356-47ac-aa3a-66361fbca4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a table showing distribution of customer count\n",
    "\n",
    "QUERY = (\n",
    "\"\"\"\n",
    "-- Step 1: Aggregate the number of customers per subcategory count\n",
    "SELECT\n",
    "  subcategory_count,\n",
    "  COUNT(cust_id) AS customer_count\n",
    "FROM (\n",
    "  -- Step 2: Count the number of unique subcategories each customer has purchased\n",
    "  SELECT\n",
    "    t.cust_id,\n",
    "    COUNT(DISTINCT p.prod_subcategory) AS subcategory_count\n",
    "  FROM\n",
    "    `machine_learning.transactions_clean` AS t\n",
    "  JOIN\n",
    "    `machine_learning.products` AS p\n",
    "  ON\n",
    "    t.prod_id = p.prod_id\n",
    "  GROUP BY\n",
    "    t.cust_id\n",
    ")\n",
    "GROUP BY\n",
    "  subcategory_count\n",
    "ORDER BY\n",
    "  subcategory_count;\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "query_job = client.query(QUERY)  # API request\n",
    "distribution = query_job.to_dataframe()  # Converts the query result directly into a pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb567961-6cfd-436a-aa98-ef45e12b5227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Draw a graph of the distribution of Customer Count\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plotting distribution of customer_count\n",
    "ax = sns.histplot(distribution['customer_count'], bins=15, kde=True)\n",
    "plt.title('Distribution of Customer Count')\n",
    "\n",
    "# Setting x-axis to start from 1\n",
    "ax.set_xlim(left=1)\n",
    "\n",
    "# Setting y-axis to start from 1\n",
    "ax.set_ylim(bottom=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc44608-471c-4230-a86e-0e147d483535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the join on 'prod_id'\n",
    "merged_data = pd.merge(sample_trans, p_table, on='prod_id', suffixes=('_trans', '_prod'))\n",
    "\n",
    "# Step 1: Count the number of unique subcategories each customer has purchased\n",
    "cust_subcategory_count = merged_data.groupby('cust_id')['prod_subcategory'].nunique().reset_index()\n",
    "cust_subcategory_count.rename(columns={'prod_subcategory': 'subcategory_count'}, inplace=True)\n",
    "\n",
    "# Step 2: Aggregate the number of customers per subcategory count\n",
    "customer_count_per_subcategory = cust_subcategory_count.groupby('subcategory_count').cust_id.count().reset_index()\n",
    "customer_count_per_subcategory.rename(columns={'cust_id': 'customer_count'}, inplace=True)\n",
    "\n",
    "# Order by subcategory_count\n",
    "customer_count_per_subcategory.sort_values(by='subcategory_count', inplace=True)\n",
    "\n",
    "\n",
    "# Draw a graph of the Distribution of Customer Count by Sub-category\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plotting distribution of customer_count by sub-category\n",
    "ax = sns.histplot(customer_count_per_subcategory['customer_count'], bins=15, kde=True)\n",
    "plt.title('Distribution of Customer Count by Sub-category')\n",
    "\n",
    "# Setting x-axis to start from 1\n",
    "ax.set_xlim(left=1)\n",
    "\n",
    "# Setting y-axis to start from 1\n",
    "ax.set_ylim(bottom=1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805c6b2-2199-4412-a35e-9d1440ebaab6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f69b99a-c5a9-4716-9a9c-6f1437d179a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = (\n",
    "\"\"\"\n",
    "SELECT *\n",
    "FROM `machine_learning.products`\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "query_job = client.query(QUERY)  # API request\n",
    "products = query_job.to_dataframe()  # Converts the query result directly into a pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af2c9c7-6fcc-4550-8cd5-e1576b0dfeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of product IDs to be dropped\n",
    "drop_prod_ids = [20640707002, 20640707004, 20313716, 20318643]\n",
    "\n",
    "# Drop specified product IDs\n",
    "products = products[~products['prod_id'].isin(drop_prod_ids)]\n",
    "\n",
    "# Update units of measure and corresponding values\n",
    "# Create a mask for rows where unit of measure is KG or L\n",
    "mask_kg = products['prod_count_uom'] == 'KG'\n",
    "mask_l = products['prod_count_uom'] == 'L'\n",
    "\n",
    "# Update 'prod_count_uom' for KG and L\n",
    "products.loc[mask_kg, 'prod_count_uom'] = 'G'\n",
    "products.loc[mask_l, 'prod_count_uom'] = 'ml'\n",
    "\n",
    "# Update 'prod_uom_value' by multiplying by 1000 where the unit was KG or L\n",
    "products.loc[mask_kg, 'prod_uom_value'] *= 1000\n",
    "products.loc[mask_l, 'prod_uom_value'] *= 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00974976-10a0-445f-b919-f961d8c1c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join both tables: products and sample_transactions\n",
    "merged_data = pd.merge(sampled_transactions, products, on=\"prod_id\", how=\"left\")\n",
    "\n",
    "# Select General Mills customers\n",
    "genm_customers = merged_data[merged_data['prod_mfc_brand_cd'] == 'GENM']['cust_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fb0111-2bf2-4838-9ef2-7a09de1ed006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter down to target customers\n",
    "acse_subcategories = ['Breakfast', 'Halloween', 'Cereal Rte', 'Coating Mixes', 'Crackers/Health Cake', 'Nutritional Portable']\n",
    "acse_transactions = merged_data[(merged_data['prod_mfc_brand_cd'] == 'ACSE') & (merged_data['prod_subcategory'].isin(acse_subcategories))]\n",
    "acse_customers = acse_transactions['cust_id'].unique()\n",
    "\n",
    "target_customers = [customer for customer in genm_customers if customer not in acse_customers]\n",
    "\n",
    "# Filter the joined table and only keep our target customers\n",
    "filtered_transactions = merged_data[merged_data['cust_id'].isin(target_customers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ce37f8-ecc4-4e0b-8886-1d9475082bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the transaction date is a datetime object and conduct train-test split\n",
    "sampled_transactions['trans_dt'] = pd.to_datetime(sampled_transactions['trans_dt'])\n",
    "\n",
    "# Define the date ranges\n",
    "start_date_train = pd.Timestamp('2019-01-01')\n",
    "end_date_train = pd.Timestamp('2019-09-30')\n",
    "start_date_test = pd.Timestamp('2019-10-01')\n",
    "end_date_test = pd.Timestamp('2019-12-31')\n",
    "\n",
    "# Filter transactions for target customers\n",
    "target_transactions = sampled_transactions[sampled_transactions['cust_id'].isin(target_customers)]\n",
    "\n",
    "# Split the data into train and test based on the date ranges\n",
    "train = target_transactions[(target_transactions['trans_dt'] >= start_date_train) & (target_transactions['trans_dt'] <= end_date_train)]\n",
    "test = target_transactions[(target_transactions['trans_dt'] >= start_date_test) & (target_transactions['trans_dt'] <= end_date_test)]\n",
    "\n",
    "# Identify customers present in both datasets\n",
    "train_customers = train['cust_id'].unique()\n",
    "test_customers = test['cust_id'].unique()\n",
    "common_customers = [customer for customer in train_customers if customer in test_customers]\n",
    "\n",
    "# Further filter train and test datasets to include only common customers\n",
    "train = train[train['cust_id'].isin(common_customers)]\n",
    "test = test[test['cust_id'].isin(common_customers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b1c8f2-aad2-430c-bd01-7676131a246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train dataset with product details\n",
    "train_with_products = pd.merge(train, products, on=\"prod_id\", how=\"left\")\n",
    "df_train = pd.DataFrame(train_with_products)\n",
    "\n",
    "# Merge test dataset with product details\n",
    "test_with_products = pd.merge(test, products, on=\"prod_id\", how=\"left\")\n",
    "df_test = pd.DataFrame(test_with_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6849178-d4ea-4e6a-a3d2-b7ebdff27db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert date into month and dayofweek to capture seasonality\n",
    "df_train['trans_dt'] = pd.to_datetime(df_train['trans_dt'])\n",
    "df_train['month'] = df_train['trans_dt'].dt.month\n",
    "df_train['dayofweek'] = df_train['trans_dt'].dt.dayofweek\n",
    "\n",
    "df_test['trans_dt'] = pd.to_datetime(df_test['trans_dt'])\n",
    "df_test['month'] = df_test['trans_dt'].dt.month\n",
    "df_test['dayofweek'] = df_test['trans_dt'].dt.dayofweek\n",
    "\n",
    "# Select categories that both appear in General Mills and Kellogg\n",
    "top_subcategories = ['Cereal Rte', 'Nutritional Portable']\n",
    "\n",
    "# Filter data to only include transactions from these subcategories\n",
    "df_train = df_train[df_train['prod_subcategory'].isin(top_subcategories)]\n",
    "df_test = df_test[df_test['cust_id'].isin(df_train['cust_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c09528-a15e-4658-9708-192033c255a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# More EDA on Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7645d3-941e-4dee-be0a-bbdf390a4cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find targeted product sub-categories\n",
    "QUERY =\n",
    "\"\"\"\n",
    "  SELECT distinct p.prod_subcategory\n",
    "  FROM `machine_learning.products` p \n",
    "  WHERE p.prod_mfc_brand_cd = 'GENM'\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "query_job = client.query(QUERY)  # API request\n",
    "\n",
    "# Save the query result to a DataFrame\n",
    "Genm_sub_cate = query_job.to_dataframe()  # Waits for query to finish and converts it to DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069b2c94-cdca-425e-a3d6-07dbbe6d9970",
   "metadata": {},
   "source": [
    "### EDA to Identify Full-Value Customers and Cherry Pickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa57d77-6df8-4190-a99b-fab5106d76a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_train.copy()\n",
    "\n",
    "cereal_data = data[data['prod_subcategory'] == 'Cereal Rte']\n",
    "nutritional_data = data[data['prod_subcategory'] == 'Nutritional Portable']\n",
    "\n",
    "# Calculate the total spending per customer for each subcategory\n",
    "cereal_spending = cereal_data.groupby('cust_id')['sales_amt'].sum()\n",
    "nutritional_spending = nutritional_data.groupby('cust_id')['sales_amt'].sum()\n",
    "\n",
    "# Calculate the variance of spending for each customer in each subcategory\n",
    "cereal_variance = cereal_spending.var()\n",
    "nutritional_variance = nutritional_spending.var()\n",
    "\n",
    "# Display the variances\n",
    "print('Variance in spending on Cereal Rte:', cereal_variance)\n",
    "print('Variance in spending on Nutritional Portable:', nutritional_variance)\n",
    "\n",
    "# Plotting the distribution of spending variance for \"Cereal Rte\"\n",
    "sns.histplot(cereal_spending, kde=True, color='blue', label='Cereal Rte')\n",
    "plt.xlabel('Number of Customers')\n",
    "plt.ylabel('Variance in Spending')\n",
    "plt.title('Distribution of Customer Spending on Cereal Rte')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the distribution of spending variance for \"Nutritional Portable\"\n",
    "sns.histplot(nutritional_spending, kde=True, color='green', label='Nutritional Portable')\n",
    "plt.xlabel('Number of Customers')\n",
    "plt.ylabel('Variance in Spending')\n",
    "plt.title('Distribution of Customer Spending on Nutritional Portable')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15211c8-598b-43ee-a900-a3b75703aa08",
   "metadata": {},
   "source": [
    "### EDA to Identify Customers with Low-Purchasing Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9085b445-04d3-470e-8c01-9a009b3cc1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the frequency of purchases per customer for each subcategory\n",
    "cereal_frequency = cereal_data.groupby('cust_id').size()\n",
    "nutritional_frequency = nutritional_data.groupby('cust_id').size()\n",
    "\n",
    "# Display the first few entries of the frequency data\n",
    "print('Frequency data for Cereal Rte:')\n",
    "print(cereal_frequency.head())\n",
    "print('Frequency data for Nutritional Portable:')\n",
    "print(nutritional_frequency.head())\n",
    "\n",
    "# Plotting the distribution of purchase frequency for \"Cereal Rte\"\n",
    "sns.histplot(cereal_frequency, kde=True, color='blue', label='Cereal Rte')\n",
    "plt.xlabel('Number of Customers')\n",
    "plt.ylabel('Purchase Frequency')\n",
    "plt.title('Distribution of Purchase Frequency on Cereal Rte')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the distribution of purchase frequency for \"Nutritional Portable\"\n",
    "sns.histplot(nutritional_frequency, kde=True, color='green', label='Nutritional Portable')\n",
    "plt.xlabel('Number of Customers')\n",
    "plt.ylabel('Purchase Frequency')\n",
    "plt.title('Distribution of Purchase Frequency on Nutritional Portable')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935b1fbf-c3c4-4f72-aacd-509b460a0723",
   "metadata": {},
   "source": [
    "### EDA to Identify Customers with Low Spending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb98784-0cd5-4a07-9d4f-f1fd2c3dbb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total spending per customer for each subcategory\n",
    "cereal_spending = cereal_data.groupby('cust_id')['sales_amt'].sum()\n",
    "nutritional_spending = nutritional_data.groupby('cust_id')['sales_amt'].sum()\n",
    "\n",
    "# Display the first few entries of the spending data\n",
    "print('Total spending data for Cereal Rte:')\n",
    "print(cereal_spending.head())\n",
    "print('Total spending data for Nutritional Portable:')\n",
    "print(nutritional_spending.head())\n",
    "\n",
    "# Plotting the distribution of total spending for \"Cereal Rte\"\n",
    "sns.histplot(cereal_spending, kde=True, color='blue', label='Cereal Rte')\n",
    "plt.xlabel('Number of Customers')\n",
    "plt.ylabel('Total Spending')\n",
    "plt.title('Distribution of Total Spending on Cereal Rte')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the distribution of total spending for \"Nutritional Portable\"\n",
    "sns.histplot(nutritional_spending, kde=True, color='green', label='Nutritional Portable')\n",
    "plt.xlabel('Number of Customers')\n",
    "plt.ylabel('Total Spending')\n",
    "plt.title('Distribution of Total Spending on Nutritional Portable')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d55813c-d396-443c-91ad-fb51cdab4e03",
   "metadata": {},
   "source": [
    "### EDA to Identify Customers with Low Average Spending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba268b7-8caa-4810-bf32-778fab97997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total spending per customer for each subcategory\n",
    "cereal_spending = cereal_data.groupby('cust_id')['sales_amt'].mean()\n",
    "nutritional_spending = nutritional_data.groupby('cust_id')['sales_amt'].mean()\n",
    "\n",
    "# Display the first few entries of the spending data\n",
    "print('Total spending data for Cereal Rte:')\n",
    "print(cereal_spending.head())\n",
    "print('Total spending data for Nutritional Portable:')\n",
    "print(nutritional_spending.head())\n",
    "\n",
    "# Plotting the distribution of total spending for \"Cereal Rte\"\n",
    "sns.histplot(cereal_spending, kde=True, color='blue', label='Cereal Rte')\n",
    "plt.xlabel('Number of Customers')\n",
    "plt.ylabel('Total Spending')\n",
    "plt.title('Distribution of Total Spending on Cereal Rte')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the distribution of total spending for \"Nutritional Portable\"\n",
    "sns.histplot(nutritional_spending, kde=True, color='green', label='Nutritional Portable')\n",
    "plt.xlabel('Number of Customers')\n",
    "plt.ylabel('Total Spending')\n",
    "plt.title('Distribution of Total Spending on Nutritional Portable')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d13279e-3dbe-483b-9e88-1d22ea49a2bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6011973-a269-42dc-9519-a63699cc6512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = df_train.copy()\n",
    "\n",
    "# Define the subcategories\n",
    "subcategories = ['Cereal Rte', 'Nutritional Portable']\n",
    "\n",
    "# Initialize dictionaries to store data\n",
    "spending_sum_data = {}\n",
    "frequency_data = {}\n",
    "spending_variance_data = {}\n",
    "average_spending_data = {}\n",
    "\n",
    "# Calculate total spending, purchase frequency, spending variance, and average spending for each customer in each subcategory\n",
    "for subcat in subcategories:\n",
    "    subcat_data = data[data['prod_subcategory'] == subcat]\n",
    "    grouped_data = subcat_data.groupby('cust_id')['sales_amt']\n",
    "    spending_sum_data[subcat] = grouped_data.sum()\n",
    "    frequency_data[subcat] = grouped_data.size()\n",
    "    spending_variance_data[subcat] = grouped_data.var().fillna(0)\n",
    "    average_spending_data[subcat] = grouped_data.mean()\n",
    "\n",
    "# Calculate the 20th percentile thresholds for sum of spending, frequency, and variance\n",
    "spending_sum_thresholds = {subcat: spending_sum_data[subcat].quantile(0.2) for subcat in subcategories}\n",
    "frequency_thresholds = {subcat: frequency_data[subcat].quantile(0.2) for subcat in subcategories}\n",
    "variance_thresholds = {subcat: spending_variance_data[subcat].quantile(0.2) for subcat in subcategories}\n",
    "\n",
    "# Calculate the mean and standard deviation of average spending\n",
    "average_spending_mean = {subcat: average_spending_data[subcat].mean() for subcat in subcategories}\n",
    "average_spending_std = {subcat: average_spending_data[subcat].std() for subcat in subcategories}\n",
    "\n",
    "# Define features based on spending and frequency thresholds\n",
    "for subcat in subcategories:\n",
    "    data[f'{subcat}_high_spending_longtail'] = data['cust_id'].map(lambda x: 1 if x in spending_sum_data[subcat] and spending_sum_data[subcat][x] > spending_sum_thresholds[subcat] else 0)\n",
    "    data[f'{subcat}_high_frequency_longtail'] = data['cust_id'].map(lambda x: 1 if x in frequency_data[subcat] and frequency_data[subcat][x] > frequency_thresholds[subcat] else 0)\n",
    "\n",
    "# Define the No_Full_value_customer feature based on variance threshold and average spending above 1 std\n",
    "for subcat in subcategories:\n",
    "    data[f'{subcat}_No_Full_value_customer'] = data['cust_id'].apply(\n",
    "        lambda x: 0 if (\n",
    "            x in spending_variance_data[subcat] and\n",
    "            spending_variance_data[subcat][x] <= variance_thresholds[subcat] and\n",
    "            x in average_spending_data[subcat] and\n",
    "            average_spending_data[subcat][x] > (average_spending_mean[subcat] + average_spending_std[subcat])\n",
    "        ) else 1\n",
    "    )\n",
    "\n",
    "# Display the first few rows to verify the new features\n",
    "print(data.head())\n",
    "\n",
    "# Update the training dataset\n",
    "df_train = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490c2420-24ba-494f-974a-061e02879c7c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Naive Bayesian Recommendation System Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5a0be9-5e5e-40f8-b87d-4bc8b3ad4332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Load the training dataset\n",
    "NB_train = df_train.copy()\n",
    "\n",
    "# Combine the relevant columns from both datasets for fitting\n",
    "all_categories = pd.concat([NB_train['prod_subcategory'], df_test['prod_subcategory']]).unique()\n",
    "label_encoder.fit(all_categories)\n",
    "\n",
    "NB_train['prod_subcategory_encoded'] = label_encoder.transform(NB_train['prod_subcategory'])\n",
    "df_test['prod_subcategory_encoded'] = label_encoder.transform(df_test['prod_subcategory'])\n",
    "\n",
    "# Train the model on available features that are useful for prediction\n",
    "model = MultinomialNB()\n",
    "features = ['cust_id', 'month','dayofweek','prod_subcategory_encoded', 'sales_amt', 'prod_uom_value', \"Cereal Rte_high_spending_longtail\", \n",
    "            \"Cereal Rte_high_frequency_longtail\", \"Cereal Rte_No_Full_value_customer\", \"Nutritional Portable_high_spending_longtail\", \n",
    "            \"Nutritional Portable_high_frequency_longtail\", \"Nutritional Portable_No_Full_value_customer\"]  \n",
    "X_train = NB_train[features]\n",
    "y_train = NB_train['prod_id']\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Filter only Kellogg's products for recommendation\n",
    "kellogg_products = NB_train[NB_train['prod_mfc_brand_cd'] == \"KLGS\"]['prod_id'].unique()\n",
    "\n",
    "# Predict probabilities for the training dataset\n",
    "train_probs = model.predict_proba(X_train)\n",
    "\n",
    "# Map probabilities to product IDs, focusing only on Kellogg's products\n",
    "product_ids = model.classes_  # array of all possible product IDs predicted by the model\n",
    "kellogg_indices = [i for i, prod_id in enumerate(product_ids) if prod_id in kellogg_products]\n",
    "\n",
    "# Extracting top 5 Kellogg's recommendations for each customer\n",
    "top_5_recommendations = pd.DataFrame(train_probs[:, kellogg_indices], index=train_df.index, columns=kellogg_products).apply(lambda x: x.nlargest(5).index.tolist(), axis=1)\n",
    "NB_train['top_5_recommendations'] = top_5_recommendations\n",
    "\n",
    "# Print out the recommendations output using Naive Bayesian Model\n",
    "NB_recommendations = NB_train[['cust_id', 'top_5_recommendations']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163620f0-c220-4091-9dfd-125c6c40e833",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Content-Based Recommendation System Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a3ae98-534f-47f1-8d4f-90b2759454e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "CB_train = df_train.copy()\n",
    "\n",
    "# Combine product features for the modeling process\n",
    "combined_features = ['cust_id', 'prod_subcategory', 'sales_amt', 'prod_uom_value', \n",
    "                     'Cereal Rte_high_spending_longtail', 'Cereal Rte_high_frequency_longtail', \n",
    "                     'Cereal Rte_No_Full_value_customer', 'Nutritional Portable_high_spending_longtail', \n",
    "                     'Nutritional Portable_high_frequency_longtail','Nutritional Portable_No_Full_value_customer']\n",
    "\n",
    "# Adjusting for potential non-string (including NaN) values in product features\n",
    "CB_train = CB_train[combined_features].fillna('').astype(str).apply(lambda x: ' '.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f553b5f-67e0-4dae-9c3c-faffbe96d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "\n",
    "# Step 1: Vectorize the combined features\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(CB_train)\n",
    "\n",
    "# Step 2: Compute cosine similarity matrix for the products\n",
    "product_similarity = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Step 3: Recommend products for each customer based on their previous interactions\n",
    "# We'll first need a mapping from indices to product IDs\n",
    "prod_index_to_id = CB_train['prod_id'].tolist()\n",
    "prod_id_to_index = {pid: idx for idx, pid in enumerate(prod_index_to_id)}\n",
    "\n",
    "# Create a dictionary to hold the recommendations for each customer\n",
    "recommendations = defaultdict(list)\n",
    "\n",
    "# Iterate through each transaction in the training data\n",
    "for index, row in df_train.iterrows():\n",
    "    cust_id = row['cust_id']\n",
    "    prod_id = row['prod_id']\n",
    "    current_prod_index = prod_id_to_index[prod_id]\n",
    "\n",
    "    # Get similarity scores for this product with all others\n",
    "    similarity_scores = list(enumerate(product_similarity[current_prod_index]))\n",
    "\n",
    "    # Sort the products based on the similarity scores\n",
    "    sorted_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top 5 product indices\n",
    "    top_product_indices = [prod_index for prod_index, score in sorted_scores[1:6]]\n",
    "\n",
    "    # Map indices back to product IDs\n",
    "    recommended_prod_ids = [prod_index_to_id[i] for i in top_product_indices]\n",
    "\n",
    "    # Add to recommendations for the customer\n",
    "    recommendations[cust_id].extend(recommended_prod_ids)\n",
    "\n",
    "# Ensure distinct and take only top 5 per customer\n",
    "for cust_id in recommendations:\n",
    "    recommendations[cust_id] = list(dict.fromkeys(recommendations[cust_id]))[:5]\n",
    "\n",
    "# Step 4: Prepare the output DataFrame\n",
    "CB_recommendations = pd.DataFrame([(cust, pid) for cust, prods in recommendations.items() for pid in prods], \n",
    "                         columns=['cust_id', 'top_5_recommendations'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f6336d-7b72-4668-95b3-8f69f2de5a78",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Collaborative Filtering Recommendation System Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2975c3d-2cf3-4cd6-bf66-50476a4de010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load training table\n",
    "CF_train = df_train.copy()\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the 'sales_qty' and 'sales_amt' columns\n",
    "CF_train[['sales_qty_normalized', 'sales_amt_normalized', 'sales_wgt_normalized']] = scaler.fit_transform(CF_train[['sales_qty', 'sales_amt', 'sales_wgt']])\n",
    "\n",
    "# Display the first few rows to verify the normalization\n",
    "CF_train[['sales_qty', 'sales_amt','sales_wgt', 'sales_qty_normalized', 'sales_amt_normalized', 'sales_wgt_normalized']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658b56f5-667d-4dca-9b2e-5cabb57939be",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    'Cereal Rte_high_spending_longtail',\n",
    "    'Cereal Rte_high_frequency_longtail',\n",
    "    'Nutritional Portable_high_spending_longtail',\n",
    "    'Nutritional Portable_high_frequency_longtail',\n",
    "    'Cereal Rte_No_Full_value_customer',\n",
    "    'Nutritional Portable_No_Full_value_customer'\n",
    "]\n",
    "\n",
    "# Normalize the new features (this is a simple min-max normalization for illustration purposes)\n",
    "scaler = MinMaxScaler()\n",
    "CF_train[feature_cols] = scaler.fit_transform(CF_train[feature_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b769ec0-7f0d-475d-a100-6596017b7856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Item-User Matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Map customer IDs and product IDs to categorical codes\n",
    "cust_map = {cust_id: i for i, cust_id in enumerate(CF_train['cust_id'].unique())}\n",
    "prod_map = {prod_id: i for i, prod_id in enumerate(CF_train['prod_id'].unique())}\n",
    "\n",
    "# Create row (customer codes), column (product codes), and data arrays (normalized sales quantities)\n",
    "row = CF_train['cust_id'].map(cust_map)\n",
    "col = CF_train['prod_id'].map(prod_map)\n",
    "data = CF_train['sales_qty_normalized'].values\n",
    "\n",
    "weights = {\n",
    "    'sales_qty': 1,\n",
    "    'sales_amt': 1,\n",
    "    'sales_wgt': 0,\n",
    "    'feature_weight': 1  # Weight for the new binary features\n",
    "}\n",
    "\n",
    "combined_interaction_score = (\n",
    "    weights['sales_qty'] * CF_train['sales_qty_normalized'] +\n",
    "    weights['sales_amt'] * CF_train['sales_amt_normalized'] +\n",
    "    weights['sales_wgt'] * CF_train['sales_wgt_normalized'] +\n",
    "    weights['feature_weight'] * CF_train[feature_cols].sum(axis=1)  # Summing all new feature columns\n",
    ") / (len(feature_cols) + 2)  # Dividing by the total number of features to average the scores (ignore weight for now)\n",
    "\n",
    "# Now, create the interaction matrix using the combined_interaction_score\n",
    "interaction_matrix_sparse = csr_matrix(\n",
    "    (combined_interaction_score.values, (row, col)),\n",
    "    shape=(len(cust_map), len(prod_map))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5e3821-07b7-4439-992e-76152cb32dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Item-item Similarities\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Transpose the matrix to get item-user format\n",
    "item_user_matrix_sparse = interaction_matrix_sparse.T\n",
    "\n",
    "# Compute item-item cosine similarity\n",
    "item_similarity_matrix = cosine_similarity(item_user_matrix_sparse, dense_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5498ef21-5dc2-48bf-a3ca-b32225817d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for 'KLGS' items\n",
    "klgs_items = CF_train[CF_train['prod_mfc_brand_cd'] == 'KLGS']\n",
    "\n",
    "# Create a set of 'prod_id' for 'KLGS' items\n",
    "klgs_item_ids = set(klgs_items['prod_id'])\n",
    "\n",
    "# Map these to the indices used in 'prod_map'\n",
    "klgs_item_indices = {prod_map[prod_id] for prod_id in klgs_item_ids if prod_id in prod_map}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa21565b-0e00-4f47-9339-5f622d8eb0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recommendations_sparse(user_index, interaction_matrix_sparse, item_similarity_matrix, top_n, klgs_item_indices):\n",
    "    # Step 1: Identify items the user has interacted with\n",
    "    interacted_indices = interaction_matrix_sparse.getrow(user_index).nonzero()[1]\n",
    "\n",
    "    # Step 2: Aggregate similar items' scores\n",
    "    item_scores = {}\n",
    "    for item_idx in interacted_indices:\n",
    "        # Retrieve the similarity scores for this item against all others\n",
    "        similar_items_scores = item_similarity_matrix.getrow(item_idx).toarray().ravel()\n",
    "\n",
    "        # Iterate through each item's similarity score to this item\n",
    "        for similar_item_idx, score in enumerate(similar_items_scores):\n",
    "            if score <= 0 or similar_item_idx not in klgs_item_indices:\n",
    "                continue  # Skip if no similarity or item is not a 'KLGS' item\n",
    "            if similar_item_idx in interacted_indices:\n",
    "                continue  # Skip items the user has already interacted with\n",
    "\n",
    "            item_scores[similar_item_idx] = item_scores.get(similar_item_idx, 0) + score\n",
    "\n",
    "    # Step 3: Rank and recommend the top N items\n",
    "    recommended_items_indices = sorted(item_scores.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "    # Map indices back to product IDs if necessary\n",
    "    recommended_items_ids = [list(prod_map.keys())[list(prod_map.values()).index(idx)] for idx, _ in recommended_items_indices]\n",
    "\n",
    "    return recommended_items_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f615d96d-2198-4564-af4e-4397ae4cb065",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = []\n",
    "for user_id in random_selected_users:\n",
    "    if user_id in cust_map:  # Ensure the user is in the cust_map\n",
    "        user_index = cust_map[user_id]  # Get user index\n",
    "        # Generate top 5 recommendations\n",
    "        top_n_recommendations = make_recommendations_sparse(user_index, interaction_matrix_sparse, item_similarity_matrix, top_n=5, klgs_item_indices=klgs_item_indices)\n",
    "        # Store the recommendations\n",
    "        recommendations.append({\"cust_id\": user_id, \"recommended_items\": top_n_recommendations})\n",
    "    else:\n",
    "        # Handle case where user ID is not found in cust_map\n",
    "        recommendations.append({\"cust_id\": user_id, \"recommended_items\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf95dab-b17f-47f9-b3f1-1e7a22b0688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll create a new DataFrame, 'recommendations_expanded_df', with the expanded format\n",
    "recommendations_expanded = []\n",
    "\n",
    "for entry in recommendations:\n",
    "    cust_id = entry['cust_id']\n",
    "    for rec_item in entry['recommended_items']:\n",
    "        recommendations_expanded.append({'cust_id': cust_id, 'top_5_recommendations': rec_item})\n",
    "\n",
    "# Convert the expanded list to a DataFrame\n",
    "CF_recommendations = pd.DataFrame(recommendations_expanded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7a8c42-7097-4f55-8183-77af88f14233",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244e54d3-3fb8-4af1-97e2-738a4257ffca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the testing table\n",
    "test_df = df_test.copy()\n",
    "\n",
    "NB_test = test_df[test_df['cust_id'].isin(NB_recommendations['cust_id'])]\n",
    "CB_test = test_df[test_df['cust_id'].isin(CB_recommendations['cust_id'])]\n",
    "CF_test = test_df[test_df['cust_id'].isin(CF_recommendations['cust_id'])]\n",
    "\n",
    "# Merge test data to include recommendations\n",
    "NB_test = NB_test.merge(NB_recommendations, on='cust_id', how='left')\n",
    "CB_test = CB_test.merge(CB_recommendations, on='cust_id', how='left')\n",
    "CF_test = CF_test.merge(CF_recommendations, on='cust_id', how='left')\n",
    "\n",
    "# Create a column to check if each purchased product is in the top 5 recommendations\n",
    "# Check if 'top_5_recommendations' is a list and handle NaN values\n",
    "NB_test['is_purchased'] = NB_test.apply(lambda row: row['prod_id'] in row['top_5_recommendations'] if isinstance(row['top_5_recommendations'], list) else False, axis=1)\n",
    "CB_test['is_purchased'] = CB_test.apply(lambda row: row['prod_id'] in row['top_5_recommendations'] if isinstance(row['top_5_recommendations'], list) else False, axis=1)\n",
    "CF_test['is_purchased'] = CF_test.apply(lambda row: row['prod_id'] in row['top_5_recommendations'] if isinstance(row['top_5_recommendations'], list) else False, axis=1)\n",
    "\n",
    "# Calculate accuracy for each customer and the average accuracy\n",
    "NB_customer_accuracy = NB_test.groupby('cust_id')['is_purchased'].sum().reset_index(name='purchases')\n",
    "NB_customer_accuracy['accuracy'] = NB_customer_accuracy['purchases'] / 5\n",
    "NB_average_accuracy = NB_customer_accuracy['accuracy'].mean()\n",
    "\n",
    "CB_customer_accuracy = CB_test.groupby('cust_id')['is_purchased'].sum().reset_index(name='purchases')\n",
    "CB_customer_accuracy['accuracy'] = CB_customer_accuracy['purchases'] / 5\n",
    "CB_average_accuracy = CB_customer_accuracy['accuracy'].mean()\n",
    "\n",
    "CF_customer_accuracy = CF_test.groupby('cust_id')['is_purchased'].sum().reset_index(name='purchases')\n",
    "CF_customer_accuracy['accuracy'] = CF_customer_accuracy['purchases'] / 5\n",
    "CF_average_accuracy = CF_customer_accuracy['accuracy'].mean()\n",
    "\n",
    "print(f\"Naive Bayesian Average accuracy: {NB_average_accuracy}\")\n",
    "print(f\"Content-Based Average accuracy: {CB_average_accuracy}\")\n",
    "print(f\"Collaborative Filtering Average accuracy: {CF_average_accuracy}\")\n",
    "\n",
    "# Calculate hit rate for each transaction\n",
    "NB_test['hit'] = NB_test['is_purchased'].astype(int)\n",
    "NB_hit_rate = NB_test['hit'].sum() / len(NB_test)\n",
    "\n",
    "CB_test['hit'] = CB_test['is_purchased'].astype(int)\n",
    "CB_hit_rate = CB_test['hit'].sum() / len(CB_test)\n",
    "\n",
    "CF_test['hit'] = CF_test['is_purchased'].astype(int)\n",
    "CF_hit_rate = CF_test['hit'].sum() / len(CF_test)\n",
    "\n",
    "print('')\n",
    "print(f\"Naive Bayesian Transaction-based hit rate: {NB_hit_rate}\")\n",
    "print(f\"Content-Based Transaction-based hit rate: {CB_hit_rate}\")\n",
    "print(f\"Collaborative Filtering Transaction-based hit rate: {CF_hit_rate}\")\n",
    "\n",
    "# Calculate the proportion of customers who bought recommended products\n",
    "NB_customers_with_hits = NB_test[NB_test['is_purchased']].drop_duplicates('cust_id').shape[0]\n",
    "NB_total_customers = NB_recommendations.shape[0]\n",
    "NB_purchase_rate = NB_customers_with_hits / NB_total_customers\n",
    "\n",
    "CB_customers_with_hits = CB_test[CB_test['is_purchased']].drop_duplicates('cust_id').shape[0]\n",
    "CB_total_customers = CB_recommendations.shape[0]\n",
    "CB_purchase_rate = CB_customers_with_hits / CB_total_customers\n",
    "\n",
    "CF_customers_with_hits = CF_test[NB_test['is_purchased']].drop_duplicates('cust_id').shape[0]\n",
    "CF_total_customers = CF_recommendations.shape[0]\n",
    "CF_purchase_rate = CF_customers_with_hits / CF_total_customers\n",
    "\n",
    "print('')\n",
    "print(f\"Naive Bayesian Customer-based purchase rate: {NB_purchase_rate}\")\n",
    "print(f\"Content-Based Customer-based purchase rate: {CB_purchase_rate}\")\n",
    "print(f\"Collaborative Filtering Customer-based purchase rate: {CF_purchase_rate}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3963851-6855-45cf-85bd-03dfa413942a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Profit Estimation and Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6fb079-1bc4-47eb-9d22-20bb25de95a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = (\n",
    "\"\"\"\n",
    "SELECT sum(sales_amt)\n",
    "FROM `machine_learning.transactions_clean` t JOIN `machine_learning.products` p ON t.prod_id = p.prod_id\n",
    "WHERE prod_mfc_brand_cd = 'GENM' AND trans_dt between '2017-01-01' AND '2019-12-31' AND (prod_subcategory = 'Cereal Rte' OR prod_subcategory = 'Nutritional Portable')\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "query_job = client.query(QUERY)  # API request\n",
    "profit = query_job.to_dataframe()  # Converts the query result directly into a pandas DataFrame\n",
    "\n",
    "# Optional: Display the first few rows to confirm it's what you expect\n",
    "profit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00adc804-dd98-402f-9cbd-bab739a8a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "data = df_train.copy()\n",
    "\n",
    "# Define the subcategories\n",
    "subcategories = ['Cereal Rte', 'Nutritional Portable']\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "proportion_of_all_features_one = {}\n",
    "\n",
    "# Iterate through each subcategory to calculate the proportion\n",
    "for subcat in subcategories:\n",
    "    # Filter data for the current subcategory\n",
    "    subcat_data = data[data['prod_subcategory'] == subcat]\n",
    "\n",
    "    # Calculate the proportion of customers with all three features as 1\n",
    "    # Count the number of customers where all three features are 1\n",
    "    count_all_features_one = subcat_data[\n",
    "        (subcat_data[f'{subcat}_high_spending_longtail'] == 1) &\n",
    "        (subcat_data[f'{subcat}_high_frequency_longtail'] == 1) &\n",
    "        (subcat_data[f'{subcat}_No_Full_value_customer'] == 1)\n",
    "    ].groupby('cust_id').size().count()\n",
    "\n",
    "    # Count the total number of unique customers in this subcategory\n",
    "    total_customers = subcat_data['cust_id'].nunique()\n",
    "\n",
    "    # Calculate the proportion\n",
    "    proportion = count_all_features_one / total_customers if total_customers > 0 else 0\n",
    "    proportion_of_all_features_one[subcat] = proportion\n",
    "\n",
    "# Print the results for each subcategory\n",
    "for subcat, proportion in proportion_of_all_features_one.items():\n",
    "    print(f'Proportion of customers with all features set to 1 in {subcat}: {proportion:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeeec3e-0959-403c-9eaf-2fa9b3faf2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## construct a BigQuery client object\n",
    "client = bigquery.Client(credentials=credentials)\n",
    "\n",
    "QUERY =\"\"\"\n",
    "  SELECT sum(sales_amt)\n",
    "  FROM `machine_learning.transactions_clean` t\n",
    "  JOIN `machine_learning.products` p ON t.prod_id = p.prod_id\n",
    "  WHERE t.trans_dt <= '2019-12-31'\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "query_job = client.query(QUERY)  # API request\n",
    "\n",
    "# Save the query result to a DataFrame\n",
    "df = query_job.to_dataframe()  # Waits for query to finish and converts it to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dd676d-e927-41b8-86bf-9253cc3ae1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume df is your DataFrame\n",
    "df = pd.DataFrame({'column': [3.270555e+07]})  # Example initialization\n",
    "\n",
    "# Calculate and display in normal format\n",
    "result = df * 0.55 * 0.0168\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
